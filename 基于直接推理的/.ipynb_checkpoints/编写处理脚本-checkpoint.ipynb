{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "852841e9-c74f-4d05-a48d-7a7549f1fa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/04 22:28:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - load model from: C:\\Users\\Mikky\\Desktop\\树冠数据集\\USIS10K\\pretrain\\sam-vit-huge\\pytorch_model.bin\n",
      "06/04 22:28:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Loads checkpoint by local backend from path: C:\\Users\\Mikky\\Desktop\\树冠数据集\\USIS10K\\pretrain\\sam-vit-huge\\pytorch_model.bin\n",
      "06/04 22:28:44 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: shared_image_embedding.positional_embedding, vision_encoder.pos_embed, vision_encoder.patch_embed.projection.weight, vision_encoder.patch_embed.projection.bias, vision_encoder.layers.0.layer_norm1.weight, vision_encoder.layers.0.layer_norm1.bias, vision_encoder.layers.0.attn.rel_pos_h, vision_encoder.layers.0.attn.rel_pos_w, vision_encoder.layers.0.attn.qkv.weight, vision_encoder.layers.0.attn.qkv.bias, vision_encoder.layers.0.attn.proj.weight, vision_encoder.layers.0.attn.proj.bias, vision_encoder.layers.0.layer_norm2.weight, vision_encoder.layers.0.layer_norm2.bias, vision_encoder.layers.0.mlp.lin1.weight, vision_encoder.layers.0.mlp.lin1.bias, vision_encoder.layers.0.mlp.lin2.weight, vision_encoder.layers.0.mlp.lin2.bias, vision_encoder.layers.1.layer_norm1.weight, vision_encoder.layers.1.layer_norm1.bias, vision_encoder.layers.1.attn.rel_pos_h, vision_encoder.layers.1.attn.rel_pos_w, vision_encoder.layers.1.attn.qkv.weight, vision_encoder.layers.1.attn.qkv.bias, vision_encoder.layers.1.attn.proj.weight, vision_encoder.layers.1.attn.proj.bias, vision_encoder.layers.1.layer_norm2.weight, vision_encoder.layers.1.layer_norm2.bias, vision_encoder.layers.1.mlp.lin1.weight, vision_encoder.layers.1.mlp.lin1.bias, vision_encoder.layers.1.mlp.lin2.weight, vision_encoder.layers.1.mlp.lin2.bias, vision_encoder.layers.2.layer_norm1.weight, vision_encoder.layers.2.layer_norm1.bias, vision_encoder.layers.2.attn.rel_pos_h, vision_encoder.layers.2.attn.rel_pos_w, vision_encoder.layers.2.attn.qkv.weight, vision_encoder.layers.2.attn.qkv.bias, vision_encoder.layers.2.attn.proj.weight, vision_encoder.layers.2.attn.proj.bias, vision_encoder.layers.2.layer_norm2.weight, vision_encoder.layers.2.layer_norm2.bias, vision_encoder.layers.2.mlp.lin1.weight, vision_encoder.layers.2.mlp.lin1.bias, vision_encoder.layers.2.mlp.lin2.weight, vision_encoder.layers.2.mlp.lin2.bias, vision_encoder.layers.3.layer_norm1.weight, vision_encoder.layers.3.layer_norm1.bias, vision_encoder.layers.3.attn.rel_pos_h, vision_encoder.layers.3.attn.rel_pos_w, vision_encoder.layers.3.attn.qkv.weight, vision_encoder.layers.3.attn.qkv.bias, vision_encoder.layers.3.attn.proj.weight, vision_encoder.layers.3.attn.proj.bias, vision_encoder.layers.3.layer_norm2.weight, vision_encoder.layers.3.layer_norm2.bias, vision_encoder.layers.3.mlp.lin1.weight, vision_encoder.layers.3.mlp.lin1.bias, vision_encoder.layers.3.mlp.lin2.weight, vision_encoder.layers.3.mlp.lin2.bias, vision_encoder.layers.4.layer_norm1.weight, vision_encoder.layers.4.layer_norm1.bias, vision_encoder.layers.4.attn.rel_pos_h, vision_encoder.layers.4.attn.rel_pos_w, vision_encoder.layers.4.attn.qkv.weight, vision_encoder.layers.4.attn.qkv.bias, vision_encoder.layers.4.attn.proj.weight, vision_encoder.layers.4.attn.proj.bias, vision_encoder.layers.4.layer_norm2.weight, vision_encoder.layers.4.layer_norm2.bias, vision_encoder.layers.4.mlp.lin1.weight, vision_encoder.layers.4.mlp.lin1.bias, vision_encoder.layers.4.mlp.lin2.weight, vision_encoder.layers.4.mlp.lin2.bias, vision_encoder.layers.5.layer_norm1.weight, vision_encoder.layers.5.layer_norm1.bias, vision_encoder.layers.5.attn.rel_pos_h, vision_encoder.layers.5.attn.rel_pos_w, vision_encoder.layers.5.attn.qkv.weight, vision_encoder.layers.5.attn.qkv.bias, vision_encoder.layers.5.attn.proj.weight, vision_encoder.layers.5.attn.proj.bias, vision_encoder.layers.5.layer_norm2.weight, vision_encoder.layers.5.layer_norm2.bias, vision_encoder.layers.5.mlp.lin1.weight, vision_encoder.layers.5.mlp.lin1.bias, vision_encoder.layers.5.mlp.lin2.weight, vision_encoder.layers.5.mlp.lin2.bias, vision_encoder.layers.6.layer_norm1.weight, vision_encoder.layers.6.layer_norm1.bias, vision_encoder.layers.6.attn.rel_pos_h, vision_encoder.layers.6.attn.rel_pos_w, vision_encoder.layers.6.attn.qkv.weight, vision_encoder.layers.6.attn.qkv.bias, vision_encoder.layers.6.attn.proj.weight, vision_encoder.layers.6.attn.proj.bias, vision_encoder.layers.6.layer_norm2.weight, vision_encoder.layers.6.layer_norm2.bias, vision_encoder.layers.6.mlp.lin1.weight, vision_encoder.layers.6.mlp.lin1.bias, vision_encoder.layers.6.mlp.lin2.weight, vision_encoder.layers.6.mlp.lin2.bias, vision_encoder.layers.7.layer_norm1.weight, vision_encoder.layers.7.layer_norm1.bias, vision_encoder.layers.7.attn.rel_pos_h, vision_encoder.layers.7.attn.rel_pos_w, vision_encoder.layers.7.attn.qkv.weight, vision_encoder.layers.7.attn.qkv.bias, vision_encoder.layers.7.attn.proj.weight, vision_encoder.layers.7.attn.proj.bias, vision_encoder.layers.7.layer_norm2.weight, vision_encoder.layers.7.layer_norm2.bias, vision_encoder.layers.7.mlp.lin1.weight, vision_encoder.layers.7.mlp.lin1.bias, vision_encoder.layers.7.mlp.lin2.weight, vision_encoder.layers.7.mlp.lin2.bias, vision_encoder.layers.8.layer_norm1.weight, vision_encoder.layers.8.layer_norm1.bias, vision_encoder.layers.8.attn.rel_pos_h, vision_encoder.layers.8.attn.rel_pos_w, vision_encoder.layers.8.attn.qkv.weight, vision_encoder.layers.8.attn.qkv.bias, vision_encoder.layers.8.attn.proj.weight, vision_encoder.layers.8.attn.proj.bias, vision_encoder.layers.8.layer_norm2.weight, vision_encoder.layers.8.layer_norm2.bias, vision_encoder.layers.8.mlp.lin1.weight, vision_encoder.layers.8.mlp.lin1.bias, vision_encoder.layers.8.mlp.lin2.weight, vision_encoder.layers.8.mlp.lin2.bias, vision_encoder.layers.9.layer_norm1.weight, vision_encoder.layers.9.layer_norm1.bias, vision_encoder.layers.9.attn.rel_pos_h, vision_encoder.layers.9.attn.rel_pos_w, vision_encoder.layers.9.attn.qkv.weight, vision_encoder.layers.9.attn.qkv.bias, vision_encoder.layers.9.attn.proj.weight, vision_encoder.layers.9.attn.proj.bias, vision_encoder.layers.9.layer_norm2.weight, vision_encoder.layers.9.layer_norm2.bias, vision_encoder.layers.9.mlp.lin1.weight, vision_encoder.layers.9.mlp.lin1.bias, vision_encoder.layers.9.mlp.lin2.weight, vision_encoder.layers.9.mlp.lin2.bias, vision_encoder.layers.10.layer_norm1.weight, vision_encoder.layers.10.layer_norm1.bias, vision_encoder.layers.10.attn.rel_pos_h, vision_encoder.layers.10.attn.rel_pos_w, vision_encoder.layers.10.attn.qkv.weight, vision_encoder.layers.10.attn.qkv.bias, vision_encoder.layers.10.attn.proj.weight, vision_encoder.layers.10.attn.proj.bias, vision_encoder.layers.10.layer_norm2.weight, vision_encoder.layers.10.layer_norm2.bias, vision_encoder.layers.10.mlp.lin1.weight, vision_encoder.layers.10.mlp.lin1.bias, vision_encoder.layers.10.mlp.lin2.weight, vision_encoder.layers.10.mlp.lin2.bias, vision_encoder.layers.11.layer_norm1.weight, vision_encoder.layers.11.layer_norm1.bias, vision_encoder.layers.11.attn.rel_pos_h, vision_encoder.layers.11.attn.rel_pos_w, vision_encoder.layers.11.attn.qkv.weight, vision_encoder.layers.11.attn.qkv.bias, vision_encoder.layers.11.attn.proj.weight, vision_encoder.layers.11.attn.proj.bias, vision_encoder.layers.11.layer_norm2.weight, vision_encoder.layers.11.layer_norm2.bias, vision_encoder.layers.11.mlp.lin1.weight, vision_encoder.layers.11.mlp.lin1.bias, vision_encoder.layers.11.mlp.lin2.weight, vision_encoder.layers.11.mlp.lin2.bias, vision_encoder.layers.12.layer_norm1.weight, vision_encoder.layers.12.layer_norm1.bias, vision_encoder.layers.12.attn.rel_pos_h, vision_encoder.layers.12.attn.rel_pos_w, vision_encoder.layers.12.attn.qkv.weight, vision_encoder.layers.12.attn.qkv.bias, vision_encoder.layers.12.attn.proj.weight, vision_encoder.layers.12.attn.proj.bias, vision_encoder.layers.12.layer_norm2.weight, vision_encoder.layers.12.layer_norm2.bias, vision_encoder.layers.12.mlp.lin1.weight, vision_encoder.layers.12.mlp.lin1.bias, vision_encoder.layers.12.mlp.lin2.weight, vision_encoder.layers.12.mlp.lin2.bias, vision_encoder.layers.13.layer_norm1.weight, vision_encoder.layers.13.layer_norm1.bias, vision_encoder.layers.13.attn.rel_pos_h, vision_encoder.layers.13.attn.rel_pos_w, vision_encoder.layers.13.attn.qkv.weight, vision_encoder.layers.13.attn.qkv.bias, vision_encoder.layers.13.attn.proj.weight, vision_encoder.layers.13.attn.proj.bias, vision_encoder.layers.13.layer_norm2.weight, vision_encoder.layers.13.layer_norm2.bias, vision_encoder.layers.13.mlp.lin1.weight, vision_encoder.layers.13.mlp.lin1.bias, vision_encoder.layers.13.mlp.lin2.weight, vision_encoder.layers.13.mlp.lin2.bias, vision_encoder.layers.14.layer_norm1.weight, vision_encoder.layers.14.layer_norm1.bias, vision_encoder.layers.14.attn.rel_pos_h, vision_encoder.layers.14.attn.rel_pos_w, vision_encoder.layers.14.attn.qkv.weight, vision_encoder.layers.14.attn.qkv.bias, vision_encoder.layers.14.attn.proj.weight, vision_encoder.layers.14.attn.proj.bias, vision_encoder.layers.14.layer_norm2.weight, vision_encoder.layers.14.layer_norm2.bias, vision_encoder.layers.14.mlp.lin1.weight, vision_encoder.layers.14.mlp.lin1.bias, vision_encoder.layers.14.mlp.lin2.weight, vision_encoder.layers.14.mlp.lin2.bias, vision_encoder.layers.15.layer_norm1.weight, vision_encoder.layers.15.layer_norm1.bias, vision_encoder.layers.15.attn.rel_pos_h, vision_encoder.layers.15.attn.rel_pos_w, vision_encoder.layers.15.attn.qkv.weight, vision_encoder.layers.15.attn.qkv.bias, vision_encoder.layers.15.attn.proj.weight, vision_encoder.layers.15.attn.proj.bias, vision_encoder.layers.15.layer_norm2.weight, vision_encoder.layers.15.layer_norm2.bias, vision_encoder.layers.15.mlp.lin1.weight, vision_encoder.layers.15.mlp.lin1.bias, vision_encoder.layers.15.mlp.lin2.weight, vision_encoder.layers.15.mlp.lin2.bias, vision_encoder.layers.16.layer_norm1.weight, vision_encoder.layers.16.layer_norm1.bias, vision_encoder.layers.16.attn.rel_pos_h, vision_encoder.layers.16.attn.rel_pos_w, vision_encoder.layers.16.attn.qkv.weight, vision_encoder.layers.16.attn.qkv.bias, vision_encoder.layers.16.attn.proj.weight, vision_encoder.layers.16.attn.proj.bias, vision_encoder.layers.16.layer_norm2.weight, vision_encoder.layers.16.layer_norm2.bias, vision_encoder.layers.16.mlp.lin1.weight, vision_encoder.layers.16.mlp.lin1.bias, vision_encoder.layers.16.mlp.lin2.weight, vision_encoder.layers.16.mlp.lin2.bias, vision_encoder.layers.17.layer_norm1.weight, vision_encoder.layers.17.layer_norm1.bias, vision_encoder.layers.17.attn.rel_pos_h, vision_encoder.layers.17.attn.rel_pos_w, vision_encoder.layers.17.attn.qkv.weight, vision_encoder.layers.17.attn.qkv.bias, vision_encoder.layers.17.attn.proj.weight, vision_encoder.layers.17.attn.proj.bias, vision_encoder.layers.17.layer_norm2.weight, vision_encoder.layers.17.layer_norm2.bias, vision_encoder.layers.17.mlp.lin1.weight, vision_encoder.layers.17.mlp.lin1.bias, vision_encoder.layers.17.mlp.lin2.weight, vision_encoder.layers.17.mlp.lin2.bias, vision_encoder.layers.18.layer_norm1.weight, vision_encoder.layers.18.layer_norm1.bias, vision_encoder.layers.18.attn.rel_pos_h, vision_encoder.layers.18.attn.rel_pos_w, vision_encoder.layers.18.attn.qkv.weight, vision_encoder.layers.18.attn.qkv.bias, vision_encoder.layers.18.attn.proj.weight, vision_encoder.layers.18.attn.proj.bias, vision_encoder.layers.18.layer_norm2.weight, vision_encoder.layers.18.layer_norm2.bias, vision_encoder.layers.18.mlp.lin1.weight, vision_encoder.layers.18.mlp.lin1.bias, vision_encoder.layers.18.mlp.lin2.weight, vision_encoder.layers.18.mlp.lin2.bias, vision_encoder.layers.19.layer_norm1.weight, vision_encoder.layers.19.layer_norm1.bias, vision_encoder.layers.19.attn.rel_pos_h, vision_encoder.layers.19.attn.rel_pos_w, vision_encoder.layers.19.attn.qkv.weight, vision_encoder.layers.19.attn.qkv.bias, vision_encoder.layers.19.attn.proj.weight, vision_encoder.layers.19.attn.proj.bias, vision_encoder.layers.19.layer_norm2.weight, vision_encoder.layers.19.layer_norm2.bias, vision_encoder.layers.19.mlp.lin1.weight, vision_encoder.layers.19.mlp.lin1.bias, vision_encoder.layers.19.mlp.lin2.weight, vision_encoder.layers.19.mlp.lin2.bias, vision_encoder.layers.20.layer_norm1.weight, vision_encoder.layers.20.layer_norm1.bias, vision_encoder.layers.20.attn.rel_pos_h, vision_encoder.layers.20.attn.rel_pos_w, vision_encoder.layers.20.attn.qkv.weight, vision_encoder.layers.20.attn.qkv.bias, vision_encoder.layers.20.attn.proj.weight, vision_encoder.layers.20.attn.proj.bias, vision_encoder.layers.20.layer_norm2.weight, vision_encoder.layers.20.layer_norm2.bias, vision_encoder.layers.20.mlp.lin1.weight, vision_encoder.layers.20.mlp.lin1.bias, vision_encoder.layers.20.mlp.lin2.weight, vision_encoder.layers.20.mlp.lin2.bias, vision_encoder.layers.21.layer_norm1.weight, vision_encoder.layers.21.layer_norm1.bias, vision_encoder.layers.21.attn.rel_pos_h, vision_encoder.layers.21.attn.rel_pos_w, vision_encoder.layers.21.attn.qkv.weight, vision_encoder.layers.21.attn.qkv.bias, vision_encoder.layers.21.attn.proj.weight, vision_encoder.layers.21.attn.proj.bias, vision_encoder.layers.21.layer_norm2.weight, vision_encoder.layers.21.layer_norm2.bias, vision_encoder.layers.21.mlp.lin1.weight, vision_encoder.layers.21.mlp.lin1.bias, vision_encoder.layers.21.mlp.lin2.weight, vision_encoder.layers.21.mlp.lin2.bias, vision_encoder.layers.22.layer_norm1.weight, vision_encoder.layers.22.layer_norm1.bias, vision_encoder.layers.22.attn.rel_pos_h, vision_encoder.layers.22.attn.rel_pos_w, vision_encoder.layers.22.attn.qkv.weight, vision_encoder.layers.22.attn.qkv.bias, vision_encoder.layers.22.attn.proj.weight, vision_encoder.layers.22.attn.proj.bias, vision_encoder.layers.22.layer_norm2.weight, vision_encoder.layers.22.layer_norm2.bias, vision_encoder.layers.22.mlp.lin1.weight, vision_encoder.layers.22.mlp.lin1.bias, vision_encoder.layers.22.mlp.lin2.weight, vision_encoder.layers.22.mlp.lin2.bias, vision_encoder.layers.23.layer_norm1.weight, vision_encoder.layers.23.layer_norm1.bias, vision_encoder.layers.23.attn.rel_pos_h, vision_encoder.layers.23.attn.rel_pos_w, vision_encoder.layers.23.attn.qkv.weight, vision_encoder.layers.23.attn.qkv.bias, vision_encoder.layers.23.attn.proj.weight, vision_encoder.layers.23.attn.proj.bias, vision_encoder.layers.23.layer_norm2.weight, vision_encoder.layers.23.layer_norm2.bias, vision_encoder.layers.23.mlp.lin1.weight, vision_encoder.layers.23.mlp.lin1.bias, vision_encoder.layers.23.mlp.lin2.weight, vision_encoder.layers.23.mlp.lin2.bias, vision_encoder.layers.24.layer_norm1.weight, vision_encoder.layers.24.layer_norm1.bias, vision_encoder.layers.24.attn.rel_pos_h, vision_encoder.layers.24.attn.rel_pos_w, vision_encoder.layers.24.attn.qkv.weight, vision_encoder.layers.24.attn.qkv.bias, vision_encoder.layers.24.attn.proj.weight, vision_encoder.layers.24.attn.proj.bias, vision_encoder.layers.24.layer_norm2.weight, vision_encoder.layers.24.layer_norm2.bias, vision_encoder.layers.24.mlp.lin1.weight, vision_encoder.layers.24.mlp.lin1.bias, vision_encoder.layers.24.mlp.lin2.weight, vision_encoder.layers.24.mlp.lin2.bias, vision_encoder.layers.25.layer_norm1.weight, vision_encoder.layers.25.layer_norm1.bias, vision_encoder.layers.25.attn.rel_pos_h, vision_encoder.layers.25.attn.rel_pos_w, vision_encoder.layers.25.attn.qkv.weight, vision_encoder.layers.25.attn.qkv.bias, vision_encoder.layers.25.attn.proj.weight, vision_encoder.layers.25.attn.proj.bias, vision_encoder.layers.25.layer_norm2.weight, vision_encoder.layers.25.layer_norm2.bias, vision_encoder.layers.25.mlp.lin1.weight, vision_encoder.layers.25.mlp.lin1.bias, vision_encoder.layers.25.mlp.lin2.weight, vision_encoder.layers.25.mlp.lin2.bias, vision_encoder.layers.26.layer_norm1.weight, vision_encoder.layers.26.layer_norm1.bias, vision_encoder.layers.26.attn.rel_pos_h, vision_encoder.layers.26.attn.rel_pos_w, vision_encoder.layers.26.attn.qkv.weight, vision_encoder.layers.26.attn.qkv.bias, vision_encoder.layers.26.attn.proj.weight, vision_encoder.layers.26.attn.proj.bias, vision_encoder.layers.26.layer_norm2.weight, vision_encoder.layers.26.layer_norm2.bias, vision_encoder.layers.26.mlp.lin1.weight, vision_encoder.layers.26.mlp.lin1.bias, vision_encoder.layers.26.mlp.lin2.weight, vision_encoder.layers.26.mlp.lin2.bias, vision_encoder.layers.27.layer_norm1.weight, vision_encoder.layers.27.layer_norm1.bias, vision_encoder.layers.27.attn.rel_pos_h, vision_encoder.layers.27.attn.rel_pos_w, vision_encoder.layers.27.attn.qkv.weight, vision_encoder.layers.27.attn.qkv.bias, vision_encoder.layers.27.attn.proj.weight, vision_encoder.layers.27.attn.proj.bias, vision_encoder.layers.27.layer_norm2.weight, vision_encoder.layers.27.layer_norm2.bias, vision_encoder.layers.27.mlp.lin1.weight, vision_encoder.layers.27.mlp.lin1.bias, vision_encoder.layers.27.mlp.lin2.weight, vision_encoder.layers.27.mlp.lin2.bias, vision_encoder.layers.28.layer_norm1.weight, vision_encoder.layers.28.layer_norm1.bias, vision_encoder.layers.28.attn.rel_pos_h, vision_encoder.layers.28.attn.rel_pos_w, vision_encoder.layers.28.attn.qkv.weight, vision_encoder.layers.28.attn.qkv.bias, vision_encoder.layers.28.attn.proj.weight, vision_encoder.layers.28.attn.proj.bias, vision_encoder.layers.28.layer_norm2.weight, vision_encoder.layers.28.layer_norm2.bias, vision_encoder.layers.28.mlp.lin1.weight, vision_encoder.layers.28.mlp.lin1.bias, vision_encoder.layers.28.mlp.lin2.weight, vision_encoder.layers.28.mlp.lin2.bias, vision_encoder.layers.29.layer_norm1.weight, vision_encoder.layers.29.layer_norm1.bias, vision_encoder.layers.29.attn.rel_pos_h, vision_encoder.layers.29.attn.rel_pos_w, vision_encoder.layers.29.attn.qkv.weight, vision_encoder.layers.29.attn.qkv.bias, vision_encoder.layers.29.attn.proj.weight, vision_encoder.layers.29.attn.proj.bias, vision_encoder.layers.29.layer_norm2.weight, vision_encoder.layers.29.layer_norm2.bias, vision_encoder.layers.29.mlp.lin1.weight, vision_encoder.layers.29.mlp.lin1.bias, vision_encoder.layers.29.mlp.lin2.weight, vision_encoder.layers.29.mlp.lin2.bias, vision_encoder.layers.30.layer_norm1.weight, vision_encoder.layers.30.layer_norm1.bias, vision_encoder.layers.30.attn.rel_pos_h, vision_encoder.layers.30.attn.rel_pos_w, vision_encoder.layers.30.attn.qkv.weight, vision_encoder.layers.30.attn.qkv.bias, vision_encoder.layers.30.attn.proj.weight, vision_encoder.layers.30.attn.proj.bias, vision_encoder.layers.30.layer_norm2.weight, vision_encoder.layers.30.layer_norm2.bias, vision_encoder.layers.30.mlp.lin1.weight, vision_encoder.layers.30.mlp.lin1.bias, vision_encoder.layers.30.mlp.lin2.weight, vision_encoder.layers.30.mlp.lin2.bias, vision_encoder.layers.31.layer_norm1.weight, vision_encoder.layers.31.layer_norm1.bias, vision_encoder.layers.31.attn.rel_pos_h, vision_encoder.layers.31.attn.rel_pos_w, vision_encoder.layers.31.attn.qkv.weight, vision_encoder.layers.31.attn.qkv.bias, vision_encoder.layers.31.attn.proj.weight, vision_encoder.layers.31.attn.proj.bias, vision_encoder.layers.31.layer_norm2.weight, vision_encoder.layers.31.layer_norm2.bias, vision_encoder.layers.31.mlp.lin1.weight, vision_encoder.layers.31.mlp.lin1.bias, vision_encoder.layers.31.mlp.lin2.weight, vision_encoder.layers.31.mlp.lin2.bias, vision_encoder.neck.conv1.weight, vision_encoder.neck.layer_norm1.weight, vision_encoder.neck.layer_norm1.bias, vision_encoder.neck.conv2.weight, vision_encoder.neck.layer_norm2.weight, vision_encoder.neck.layer_norm2.bias, mask_decoder.iou_token.weight, mask_decoder.mask_tokens.weight, mask_decoder.transformer.layers.0.self_attn.q_proj.weight, mask_decoder.transformer.layers.0.self_attn.q_proj.bias, mask_decoder.transformer.layers.0.self_attn.k_proj.weight, mask_decoder.transformer.layers.0.self_attn.k_proj.bias, mask_decoder.transformer.layers.0.self_attn.v_proj.weight, mask_decoder.transformer.layers.0.self_attn.v_proj.bias, mask_decoder.transformer.layers.0.self_attn.out_proj.weight, mask_decoder.transformer.layers.0.self_attn.out_proj.bias, mask_decoder.transformer.layers.0.layer_norm1.weight, mask_decoder.transformer.layers.0.layer_norm1.bias, mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight, mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias, mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight, mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias, mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight, mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias, mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight, mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias, mask_decoder.transformer.layers.0.layer_norm2.weight, mask_decoder.transformer.layers.0.layer_norm2.bias, mask_decoder.transformer.layers.0.mlp.lin1.weight, mask_decoder.transformer.layers.0.mlp.lin1.bias, mask_decoder.transformer.layers.0.mlp.lin2.weight, mask_decoder.transformer.layers.0.mlp.lin2.bias, mask_decoder.transformer.layers.0.layer_norm3.weight, mask_decoder.transformer.layers.0.layer_norm3.bias, mask_decoder.transformer.layers.0.layer_norm4.weight, mask_decoder.transformer.layers.0.layer_norm4.bias, mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight, mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias, mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight, mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias, mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight, mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias, mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight, mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias, mask_decoder.transformer.layers.1.self_attn.q_proj.weight, mask_decoder.transformer.layers.1.self_attn.q_proj.bias, mask_decoder.transformer.layers.1.self_attn.k_proj.weight, mask_decoder.transformer.layers.1.self_attn.k_proj.bias, mask_decoder.transformer.layers.1.self_attn.v_proj.weight, mask_decoder.transformer.layers.1.self_attn.v_proj.bias, mask_decoder.transformer.layers.1.self_attn.out_proj.weight, mask_decoder.transformer.layers.1.self_attn.out_proj.bias, mask_decoder.transformer.layers.1.layer_norm1.weight, mask_decoder.transformer.layers.1.layer_norm1.bias, mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight, mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias, mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight, mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias, mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight, mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias, mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight, mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias, mask_decoder.transformer.layers.1.layer_norm2.weight, mask_decoder.transformer.layers.1.layer_norm2.bias, mask_decoder.transformer.layers.1.mlp.lin1.weight, mask_decoder.transformer.layers.1.mlp.lin1.bias, mask_decoder.transformer.layers.1.mlp.lin2.weight, mask_decoder.transformer.layers.1.mlp.lin2.bias, mask_decoder.transformer.layers.1.layer_norm3.weight, mask_decoder.transformer.layers.1.layer_norm3.bias, mask_decoder.transformer.layers.1.layer_norm4.weight, mask_decoder.transformer.layers.1.layer_norm4.bias, mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight, mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias, mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight, mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias, mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight, mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias, mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight, mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias, mask_decoder.transformer.final_attn_token_to_image.q_proj.weight, mask_decoder.transformer.final_attn_token_to_image.q_proj.bias, mask_decoder.transformer.final_attn_token_to_image.k_proj.weight, mask_decoder.transformer.final_attn_token_to_image.k_proj.bias, mask_decoder.transformer.final_attn_token_to_image.v_proj.weight, mask_decoder.transformer.final_attn_token_to_image.v_proj.bias, mask_decoder.transformer.final_attn_token_to_image.out_proj.weight, mask_decoder.transformer.final_attn_token_to_image.out_proj.bias, mask_decoder.transformer.layer_norm_final_attn.weight, mask_decoder.transformer.layer_norm_final_attn.bias, mask_decoder.upscale_conv1.weight, mask_decoder.upscale_conv1.bias, mask_decoder.upscale_conv2.weight, mask_decoder.upscale_conv2.bias, mask_decoder.upscale_layer_norm.weight, mask_decoder.upscale_layer_norm.bias, mask_decoder.output_hypernetworks_mlps.0.proj_in.weight, mask_decoder.output_hypernetworks_mlps.0.proj_in.bias, mask_decoder.output_hypernetworks_mlps.0.proj_out.weight, mask_decoder.output_hypernetworks_mlps.0.proj_out.bias, mask_decoder.output_hypernetworks_mlps.0.layers.0.weight, mask_decoder.output_hypernetworks_mlps.0.layers.0.bias, mask_decoder.output_hypernetworks_mlps.1.proj_in.weight, mask_decoder.output_hypernetworks_mlps.1.proj_in.bias, mask_decoder.output_hypernetworks_mlps.1.proj_out.weight, mask_decoder.output_hypernetworks_mlps.1.proj_out.bias, mask_decoder.output_hypernetworks_mlps.1.layers.0.weight, mask_decoder.output_hypernetworks_mlps.1.layers.0.bias, mask_decoder.output_hypernetworks_mlps.2.proj_in.weight, mask_decoder.output_hypernetworks_mlps.2.proj_in.bias, mask_decoder.output_hypernetworks_mlps.2.proj_out.weight, mask_decoder.output_hypernetworks_mlps.2.proj_out.bias, mask_decoder.output_hypernetworks_mlps.2.layers.0.weight, mask_decoder.output_hypernetworks_mlps.2.layers.0.bias, mask_decoder.output_hypernetworks_mlps.3.proj_in.weight, mask_decoder.output_hypernetworks_mlps.3.proj_in.bias, mask_decoder.output_hypernetworks_mlps.3.proj_out.weight, mask_decoder.output_hypernetworks_mlps.3.proj_out.bias, mask_decoder.output_hypernetworks_mlps.3.layers.0.weight, mask_decoder.output_hypernetworks_mlps.3.layers.0.bias, mask_decoder.iou_prediction_head.proj_in.weight, mask_decoder.iou_prediction_head.proj_in.bias, mask_decoder.iou_prediction_head.proj_out.weight, mask_decoder.iou_prediction_head.proj_out.bias, mask_decoder.iou_prediction_head.layers.0.weight, mask_decoder.iou_prediction_head.layers.0.bias, prompt_encoder.shared_embedding.positional_embedding\n",
      "\n",
      "06/04 22:28:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "prompt_encoder.mask_embed.conv1.weight - torch.Size([4, 1, 2, 2]): \n",
      "PretrainedInit: load from C:\\Users\\Mikky\\Desktop\\树冠数据集\\USIS10K\\pretrain\\sam-vit-huge\\pytorch_model.bin \n",
      " \n",
      "06/04 22:28:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "prompt_encoder.mask_embed.conv1.bias - torch.Size([4]): \n",
      "PretrainedInit: load from C:\\Users\\Mikky\\Desktop\\树冠数据集\\USIS10K\\pretrain\\sam-vit-huge\\pytorch_model.bin \n",
      " \n",
      "06/04 22:28:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "prompt_encoder.mask_embed.conv2.weight - torch.Size([16, 4, 2, 2]): \n",
      "PretrainedInit: load from C:\\Users\\Mikky\\Desktop\\树冠数据集\\USIS10K\\pretrain\\sam-vit-huge\\pytorch_model.bin \n",
      " \n",
      "06/04 22:28:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "prompt_encoder.mask_embed.conv2.bias - torch.Size([16]): \n",
      "PretrainedInit: load from C:\\Users\\Mikky\\Desktop\\树冠数据集\\USIS10K\\pretrain\\sam-vit-huge\\pytorch_model.bin \n",
      " \n",
      "06/04 22:28:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "prompt_encoder.mask_embed.conv3.weight - torch.Size([256, 16, 1, 1]): \n",
      "PretrainedInit: load from C:\\Users\\Mikky\\Desktop\\树冠数据集\\USIS10K\\pretrain\\sam-vit-huge\\pytorch_model.bin \n",
      " \n",
      "06/04 22:28:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "prompt_encoder.mask_embed.conv3.bias - torch.Size([256]): \n",
      "PretrainedInit: load from C:\\Users\\Mikky\\Desktop\\树冠数据集\\USIS10K\\pretrain\\sam-vit-huge\\pytorch_model.bin \n",
      " \n",
      "06/04 22:28:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "prompt_encoder.mask_embed.layer_norm1.weight - torch.Size([4]): \n",
      "PretrainedInit: load from C:\\Users\\Mikky\\Desktop\\树冠数据集\\USIS10K\\pretrain\\sam-vit-huge\\pytorch_model.bin \n",
      " \n",
      "06/04 22:28:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "prompt_encoder.mask_embed.layer_norm1.bias - torch.Size([4]): \n",
      "PretrainedInit: load from C:\\Users\\Mikky\\Desktop\\树冠数据集\\USIS10K\\pretrain\\sam-vit-huge\\pytorch_model.bin \n",
      " \n",
      "06/04 22:28:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "prompt_encoder.mask_embed.layer_norm2.weight - torch.Size([16]): \n",
      "PretrainedInit: load from C:\\Users\\Mikky\\Desktop\\树冠数据集\\USIS10K\\pretrain\\sam-vit-huge\\pytorch_model.bin \n",
      " \n",
      "06/04 22:28:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "prompt_encoder.mask_embed.layer_norm2.bias - torch.Size([16]): \n",
      "PretrainedInit: load from C:\\Users\\Mikky\\Desktop\\树冠数据集\\USIS10K\\pretrain\\sam-vit-huge\\pytorch_model.bin \n",
      " \n",
      "06/04 22:28:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "prompt_encoder.no_mask_embed.weight - torch.Size([1, 256]): \n",
      "PretrainedInit: load from C:\\Users\\Mikky\\Desktop\\树冠数据集\\USIS10K\\pretrain\\sam-vit-huge\\pytorch_model.bin \n",
      " \n",
      "06/04 22:28:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "prompt_encoder.point_embed.0.weight - torch.Size([1, 256]): \n",
      "PretrainedInit: load from C:\\Users\\Mikky\\Desktop\\树冠数据集\\USIS10K\\pretrain\\sam-vit-huge\\pytorch_model.bin \n",
      " \n",
      "06/04 22:28:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "prompt_encoder.point_embed.1.weight - torch.Size([1, 256]): \n",
      "PretrainedInit: load from C:\\Users\\Mikky\\Desktop\\树冠数据集\\USIS10K\\pretrain\\sam-vit-huge\\pytorch_model.bin \n",
      " \n",
      "06/04 22:28:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "prompt_encoder.point_embed.2.weight - torch.Size([1, 256]): \n",
      "PretrainedInit: load from C:\\Users\\Mikky\\Desktop\\树冠数据集\\USIS10K\\pretrain\\sam-vit-huge\\pytorch_model.bin \n",
      " \n",
      "06/04 22:28:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "prompt_encoder.point_embed.3.weight - torch.Size([1, 256]): \n",
      "PretrainedInit: load from C:\\Users\\Mikky\\Desktop\\树冠数据集\\USIS10K\\pretrain\\sam-vit-huge\\pytorch_model.bin \n",
      " \n",
      "06/04 22:28:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "prompt_encoder.not_a_point_embed.weight - torch.Size([1, 256]): \n",
      "PretrainedInit: load from C:\\Users\\Mikky\\Desktop\\树冠数据集\\USIS10K\\pretrain\\sam-vit-huge\\pytorch_model.bin \n",
      " \n",
      "Loads checkpoint by local backend from path: C:\\Users\\Mikky\\Desktop\\树冠数据集\\USIS10K\\work_dirs\\USIS10KDataset\\huge\\best_coco_bbox_mAP_epoch_27.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from mmdet.apis import init_detector\n",
    "from pycocotools.mask import encode\n",
    "\n",
    "# 初始化模型\n",
    "config_file = r'C:\\Users\\Mikky\\Desktop\\树冠数据集\\USIS10K\\work_dirs\\USIS10KDataset\\huge\\multiclass_usis_train_huge.py'\n",
    "checkpoint_file = r'C:\\Users\\Mikky\\Desktop\\树冠数据集\\USIS10K\\work_dirs\\USIS10KDataset\\huge\\best_coco_bbox_mAP_epoch_27.pth'\n",
    "model = init_detector(config_file, checkpoint_file, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f123f35-3d2e-47c8-a7a8-32342a9c77fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import prepare_model_inputs,inference_bbox,prepare_pipeline,slice_bbox\n",
    "# 假设你已经有了model实例和test_pipeline实例\n",
    "imgs = [r'C:\\Users\\Mikky\\Desktop\\树冠数据集\\第二次数据集\\Testing_set_phase1\\Testing_set\\images\\Dataset_4_val_0.png']\n",
    "img=imgs[0]\n",
    "\n",
    "test_pipeline=prepare_pipeline(model, img)\n",
    "\n",
    "inputs, data_samples = prepare_model_inputs(img, model,test_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52bf231a-142f-4b54-9dca-9373094b1c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<DetDataSample(\n",
       " \n",
       "     META INFORMATION\n",
       "     img_id: 0\n",
       "     pad_shape: (1024, 1024)\n",
       "     img_path: 'C:\\\\Users\\\\Mikky\\\\Desktop\\\\树冠数据集\\\\第二次数据集\\\\Testing_set_phase1\\\\Testing_set\\\\images\\\\Dataset_4_val_0.png'\n",
       "     scale_factor: (1.0, 1.0)\n",
       "     img_shape: (1024, 1024)\n",
       "     ori_shape: (1024, 1024)\n",
       "     batch_input_shape: (1024, 1024)\n",
       " \n",
       "     DATA FIELDS\n",
       "     gt_instances: <InstanceData(\n",
       "         \n",
       "             META INFORMATION\n",
       "         \n",
       "             DATA FIELDS\n",
       "             labels: tensor([], device='cuda:0', dtype=torch.int64)\n",
       "             bboxes: tensor([], device='cuda:0', size=(0, 4))\n",
       "             masks: BitmapMasks(num_masks=0, height=1024, width=1024)\n",
       "         ) at 0x28f96810c90>\n",
       "     ignored_instances: <InstanceData(\n",
       "         \n",
       "             META INFORMATION\n",
       "         \n",
       "             DATA FIELDS\n",
       "             labels: tensor([], device='cuda:0', dtype=torch.int64)\n",
       "             bboxes: tensor([], device='cuda:0', size=(0, 4))\n",
       "             masks: BitmapMasks(num_masks=0, height=1024, width=1024)\n",
       "         ) at 0x28f96811110>\n",
       " ) at 0x28f974513d0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "366a85c2-e891-4501-a3b5-cd2dd93dae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = inference_bbox(model, inputs, data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07fa39b4-9d52-49a6-92ff-1ef2db9b002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis import visualize_bbox_results, visualize_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b499ea0-5a58-4dfd-8c54-3d9858e8ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "visualize_bbox_results(img,data_samples[0].pred_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ce56da-1217-4177-9027-439cb6632d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54407d90-1a51-4ba6-ad4b-5df37fb0708d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing slices: 100%|███████████████████████████████████████████████████████████████| 25/25 [00:10<00:00,  2.49it/s]\n"
     ]
    }
   ],
   "source": [
    "data_sample=slice_bbox(img, model, scale_factor=3,overlap=0.5,nms=True) # , nms_cfg=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e3962a1-5582-45ee-9126-6294b5043aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[777.7944, 701.4855, 886.8213, 812.5743],\n",
       "        [482.3196, 269.0388, 565.2842, 357.8298],\n",
       "        [590.5324, 173.4079, 643.3612, 233.4373],\n",
       "        ...,\n",
       "        [558.0894, 568.8455, 582.6973, 594.2339],\n",
       "        [675.6006, 371.4516, 712.3553, 393.8386],\n",
       "        [541.1625, 683.0000, 563.7170, 692.1971]], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample.pred_instances.bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebe0e283-b419-4870-b5e5-214b73961f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([639, 4])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa8d2b86-35be-4829-a1c5-df379a9bc953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 11,  19,  28,  30,  36,  40,  45,  47,  57,  61,  75, 104, 107, 110,\n",
       "        115, 121, 133, 135, 142, 148, 154, 167, 200, 208, 242, 261, 263, 293,\n",
       "        312, 316, 317, 329, 346, 347, 357, 358, 365, 372, 373, 375, 413, 440,\n",
       "        446, 453, 485, 486, 491, 506, 532, 533, 542, 544, 565, 580, 585, 599,\n",
       "        601, 619, 624, 625], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "tensor=data_sample.pred_instances.bboxes\n",
    "# 设定边界\n",
    "x_min, x_max = 0, 341\n",
    "y_min, y_max = 0, 341\n",
    "\n",
    "# 检查条件\n",
    "valid_rows = ((tensor[:, 0] >= x_min) & (tensor[:, 0] <= x_max) &\n",
    "              (tensor[:, 2] >= x_min) & (tensor[:, 2] <= x_max) &\n",
    "              (tensor[:, 1] >= y_min) & (tensor[:, 1] <= y_max) &\n",
    "              (tensor[:, 3] >= y_min) & (tensor[:, 3] <= y_max))\n",
    "\n",
    "# 获取满足条件的行索引\n",
    "valid_row_indices = torch.where(valid_rows)[0]\n",
    "valid_row_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "217924d1-85de-494b-88a3-b7aca9303c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<InstanceData(\n",
       "\n",
       "    META INFORMATION\n",
       "\n",
       "    DATA FIELDS\n",
       "    labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
       "    bboxes: tensor([[2.7200e+02, 2.8286e+02, 3.4052e+02, 3.4081e+02],\n",
       "                [9.4735e+00, 6.0578e+01, 1.0064e+02, 1.6402e+02],\n",
       "                [2.1600e+02, 2.8392e+01, 2.9164e+02, 1.4599e+02],\n",
       "                [1.7100e+02, 1.9930e+02, 2.0903e+02, 3.0596e+02],\n",
       "                [4.0930e+01, 1.0144e+00, 1.0134e+02, 4.8583e+01],\n",
       "                [4.0916e+00, 1.7222e+02, 7.9929e+01, 2.2426e+02],\n",
       "                [9.2080e+01, 3.7851e+00, 2.1457e+02, 1.5248e+02],\n",
       "                [1.0289e+02, 1.1133e+02, 1.6213e+02, 1.6201e+02],\n",
       "                [2.0730e+02, 1.7100e+02, 2.7721e+02, 2.2214e+02],\n",
       "                [2.2497e+02, 2.3519e+02, 3.0548e+02, 3.1142e+02],\n",
       "                [8.5465e+01, 2.8417e+02, 1.4157e+02, 3.1791e+02],\n",
       "                [8.8384e+01, 1.7100e+02, 1.2319e+02, 1.9970e+02],\n",
       "                [1.7130e+02, 2.9176e-03, 1.9307e+02, 6.6730e+00],\n",
       "                [8.8756e-02, 1.8661e+01, 3.4939e+01, 7.2614e+01],\n",
       "                [1.6321e-01, 2.2126e+02, 1.8030e+01, 2.5447e+02],\n",
       "                [2.8420e+02, 5.1559e+01, 3.4055e+02, 1.1959e+02],\n",
       "                [4.3060e+01, 2.1926e+02, 1.1148e+02, 2.7155e+02],\n",
       "                [2.7317e+02, 1.7110e+02, 3.0352e+02, 1.8938e+02],\n",
       "                [9.1944e-02, 1.7270e-01, 4.8402e+01, 2.7678e+01],\n",
       "                [9.5774e+01, 2.5641e+02, 1.4824e+02, 2.9279e+02],\n",
       "                [2.9526e+02, 1.8197e+02, 3.4052e+02, 2.7388e+02],\n",
       "                [3.2652e+02, 2.4379e+01, 3.4095e+02, 7.0405e+01],\n",
       "                [6.6115e-01, 2.4553e+02, 4.7294e+01, 3.0212e+02],\n",
       "                [6.0301e-02, 3.3017e+02, 1.6363e+01, 3.4100e+02],\n",
       "                [2.5905e-01, 2.9625e+02, 5.3795e+01, 3.3937e+02],\n",
       "                [3.2445e+02, 3.0967e+02, 3.4100e+02, 3.3869e+02],\n",
       "                [7.9652e+01, 3.1296e+02, 1.2996e+02, 3.4092e+02],\n",
       "                [1.7100e+02, 4.2139e+00, 1.9107e+02, 7.8571e+01],\n",
       "                [1.8673e+02, 3.1719e+02, 2.3828e+02, 3.4024e+02],\n",
       "                [1.9612e+02, 6.2954e+00, 2.2421e+02, 3.2770e+01],\n",
       "                [2.6772e+02, 1.8315e+02, 3.0729e+02, 2.1317e+02],\n",
       "                [1.7102e+02, 1.7114e+02, 1.8790e+02, 1.8024e+02],\n",
       "                [9.0072e+01, 1.4393e+02, 1.1271e+02, 1.6679e+02],\n",
       "                [2.8978e+02, 9.9130e+01, 3.1354e+02, 1.2524e+02],\n",
       "                [2.0178e+02, 1.7102e+02, 2.2343e+02, 1.8107e+02],\n",
       "                [2.2498e+02, 1.1412e+02, 2.7647e+02, 1.4752e+02],\n",
       "                [1.9587e+02, 1.1071e+02, 2.2609e+02, 1.5269e+02],\n",
       "                [3.3480e+02, 1.7100e+02, 3.4100e+02, 1.7689e+02],\n",
       "                [3.6973e+01, 2.5782e+02, 5.4263e+01, 2.7643e+02],\n",
       "                [1.7100e+02, 1.4555e+02, 1.9435e+02, 1.8030e+02],\n",
       "                [3.5797e+01, 0.0000e+00, 5.5444e+01, 9.8564e+00],\n",
       "                [2.6695e+02, 9.3135e+01, 2.8867e+02, 1.1874e+02],\n",
       "                [5.6686e+01, 2.0502e+02, 9.2100e+01, 2.2443e+02],\n",
       "                [1.7120e+02, 1.9814e+02, 1.8661e+02, 2.3615e+02],\n",
       "                [3.1839e+00, 7.0089e+01, 4.4584e+01, 1.1200e+02],\n",
       "                [5.4973e+01, 2.2089e+02, 8.3511e+01, 2.4413e+02],\n",
       "                [2.4586e+02, 2.1151e+02, 2.6334e+02, 2.3903e+02],\n",
       "                [3.2277e+02, 1.8152e+02, 3.4078e+02, 2.2085e+02],\n",
       "                [2.3404e+02, 1.7110e+02, 2.7394e+02, 1.8770e+02],\n",
       "                [2.9381e+02, 1.7100e+02, 3.2189e+02, 1.8579e+02],\n",
       "                [1.7120e+02, 2.7474e+01, 1.7864e+02, 5.2819e+01],\n",
       "                [1.7273e+02, 3.0738e+02, 1.9469e+02, 3.3416e+02],\n",
       "                [6.4450e-01, 7.4619e-03, 2.1363e+01, 6.1181e+00],\n",
       "                [2.2409e+02, 1.7100e+02, 2.4273e+02, 1.7746e+02],\n",
       "                [1.1020e+02, 2.2066e-02, 1.3211e+02, 1.9475e+01],\n",
       "                [3.4754e-02, 1.6092e+02, 7.4562e+00, 1.8344e+02],\n",
       "                [4.3601e+01, 1.6573e+02, 1.8527e+02, 2.9955e+02],\n",
       "                [5.3472e+01, 0.0000e+00, 7.2113e+01, 7.9577e+00],\n",
       "                [2.2560e+02, 3.3682e+02, 2.5034e+02, 3.4100e+02],\n",
       "                [2.7713e+02, 1.2152e+02, 2.9335e+02, 1.4634e+02]], device='cuda:0')\n",
       "    scores: tensor([0.9852, 0.9791, 0.9705, 0.9692, 0.9630, 0.9599, 0.9556, 0.9543, 0.9461,\n",
       "                0.9413, 0.9235, 0.8850, 0.8813, 0.8775, 0.8720, 0.8675, 0.8488, 0.8435,\n",
       "                0.8316, 0.8283, 0.8204, 0.8026, 0.7220, 0.7109, 0.6001, 0.5474, 0.5462,\n",
       "                0.4831, 0.4295, 0.4260, 0.4233, 0.4082, 0.3478, 0.3438, 0.3161, 0.3140,\n",
       "                0.3002, 0.2820, 0.2801, 0.2790, 0.2178, 0.1838, 0.1765, 0.1675, 0.1422,\n",
       "                0.1417, 0.1401, 0.1280, 0.1098, 0.1080, 0.1034, 0.1014, 0.0867, 0.0817,\n",
       "                0.0770, 0.0725, 0.0724, 0.0625, 0.0600, 0.0599], device='cuda:0')\n",
       ") at 0x292b3128910>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample.pred_instances[valid_row_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2055484c-6be2-4f51-84e1-8544fa653562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18ea6f89-2238-446f-87e0-1a60cfd83748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_bbox_results(img,rpn_results_list1[0])\n",
    "# visualize_bbox_results(img,data_sample.pred_instances)\n",
    "visualize_bbox_results(img,data_sample.pred_instances[valid_row_indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52c569cb-1c3c-444e-9151-31fef9b26092",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DetDataSample(\n",
       "\n",
       "    META INFORMATION\n",
       "    img_path: None\n",
       "    pad_shape: (1024, 1024)\n",
       "    batch_input_shape: (1024, 1024)\n",
       "    img_id: 0\n",
       "    img_shape: (1024, 1024)\n",
       "    scale_factor: (3.002932551319648, 3.002932551319648)\n",
       "    ori_shape: (1024, 1024)\n",
       "\n",
       "    DATA FIELDS\n",
       "    ignored_instances: <InstanceData(\n",
       "        \n",
       "            META INFORMATION\n",
       "        \n",
       "            DATA FIELDS\n",
       "            bboxes: tensor([], device='cuda:0', size=(0, 4))\n",
       "            labels: tensor([], device='cuda:0', dtype=torch.int64)\n",
       "            masks: BitmapMasks(num_masks=0, height=341, width=341)\n",
       "        ) at 0x20b0be3fd10>\n",
       "    pred_instances: <InstanceData(\n",
       "        \n",
       "            META INFORMATION\n",
       "        \n",
       "            DATA FIELDS\n",
       "            bboxes: tensor([[  41.5716,  186.2041,  299.2260,  495.6251],\n",
       "                        [ 825.0099,    4.9234, 1007.2484,   94.5713],\n",
       "                        [ 576.9423,  389.7273,  852.7220,  636.7300],\n",
       "                        ...,\n",
       "                        [ 966.9137,  683.0000, 1433.2075,  761.9286],\n",
       "                        [ 866.2080,  715.4923, 1511.1284, 1199.8049],\n",
       "                        [ 693.6038,  683.0000,  916.0314,  943.7804]], device='cuda:0')\n",
       "            labels: tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
       "            scores: tensor([0.9999, 0.9999, 0.9999,  ..., 0.7314, 0.7154, 0.7066], device='cuda:0')\n",
       "        ) at 0x20b0be571d0>\n",
       "    gt_instances: <InstanceData(\n",
       "        \n",
       "            META INFORMATION\n",
       "        \n",
       "            DATA FIELDS\n",
       "            bboxes: tensor([], device='cuda:0', size=(0, 4))\n",
       "            labels: tensor([], device='cuda:0', dtype=torch.int64)\n",
       "            masks: BitmapMasks(num_masks=0, height=341, width=341)\n",
       "        ) at 0x20b0be3f390>\n",
       ") at 0x20b0be697d0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample.set_metainfo({\"ori_shape\":(1024,1024),\"scale_factor\":(1,1)})\n",
    "data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fbf7ba0-41e4-4ce8-b5e0-e32e2ce36192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<InstanceData(\n",
       "\n",
       "    META INFORMATION\n",
       "\n",
       "    DATA FIELDS\n",
       "    bboxes: tensor([[  41.5716,  186.2041,  299.2260,  495.6251],\n",
       "                [ 825.0099,    4.9234, 1007.2484,   94.5713],\n",
       "                [ 576.9423,  389.7273,  852.7220,  636.7300],\n",
       "                ...,\n",
       "                [ 966.9137,  683.0000, 1433.2075,  761.9286],\n",
       "                [ 866.2080,  715.4923, 1511.1284, 1199.8049],\n",
       "                [ 693.6038,  683.0000,  916.0314,  943.7804]], device='cuda:0')\n",
       "    labels: tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
       "    scores: tensor([0.9999, 0.9999, 0.9999,  ..., 0.7314, 0.7154, 0.7066], device='cuda:0')\n",
       ") at 0x20aa79aa8d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample.pred_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f8c7989-558e-4981-8b5d-5c8cb90c0869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<InstanceData(\n",
       "\n",
       "    META INFORMATION\n",
       "\n",
       "    DATA FIELDS\n",
       "    bboxes: tensor([[ 765.6517,  701.5146,  883.5161,  808.3105],\n",
       "                [ 475.9961,  949.8996,  555.0309, 1023.7498],\n",
       "                [ 570.5417,  806.4945,  641.0295,  882.1692],\n",
       "                ...,\n",
       "                [ 178.6971,  529.4672,  453.8914,  772.1473],\n",
       "                [ 603.4911,  322.8588,  852.9773,  540.7578],\n",
       "                [ 177.4955,  712.3575,  459.6915,  945.1843]], device='cuda:0')\n",
       "    labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
       "    scores: tensor([0.9999, 0.9998, 0.9996, 0.9996, 0.9995, 0.9995, 0.9995, 0.9995, 0.9993,\n",
       "                0.9993, 0.9992, 0.9992, 0.9992, 0.9992, 0.9991, 0.9991, 0.9991, 0.9991,\n",
       "                0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "                0.9989, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9987, 0.9987, 0.9987,\n",
       "                0.9987, 0.9986, 0.9986, 0.9986, 0.9986, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "                0.9985, 0.9984, 0.9984, 0.9984, 0.9983, 0.9983, 0.9983, 0.9983, 0.9983,\n",
       "                0.9982, 0.9982, 0.9982, 0.9982, 0.9982, 0.9982, 0.9982, 0.9982, 0.9982,\n",
       "                0.9981, 0.9981, 0.9981, 0.9981, 0.9981, 0.9981, 0.9980, 0.9980, 0.9980,\n",
       "                0.9980, 0.9979, 0.9979, 0.9979, 0.9979, 0.9979, 0.9978, 0.9978, 0.9978,\n",
       "                0.9978, 0.9978, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "                0.9976, 0.9976, 0.9975, 0.9975, 0.9975, 0.9975, 0.9974, 0.9974, 0.9973,\n",
       "                0.9973, 0.9972, 0.9972, 0.9972, 0.9971, 0.9971, 0.9971, 0.9971, 0.9971,\n",
       "                0.9970, 0.9970, 0.9970, 0.9970, 0.9970, 0.9970, 0.9970, 0.9969, 0.9969,\n",
       "                0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9968, 0.9968, 0.9968, 0.9968,\n",
       "                0.9967, 0.9967, 0.9966, 0.9966, 0.9966, 0.9966, 0.9965, 0.9965, 0.9965,\n",
       "                0.9965, 0.9964, 0.9964, 0.9964, 0.9963, 0.9962, 0.9962, 0.9962, 0.9961,\n",
       "                0.9961, 0.9961, 0.9961, 0.9960, 0.9960, 0.9958, 0.9958, 0.9957, 0.9957,\n",
       "                0.9957, 0.9956, 0.9956, 0.9956, 0.9955, 0.9955, 0.9955, 0.9954, 0.9954,\n",
       "                0.9954, 0.9954, 0.9954, 0.9953, 0.9953, 0.9953, 0.9953, 0.9952, 0.9952,\n",
       "                0.9952, 0.9951, 0.9951, 0.9951, 0.9950, 0.9950, 0.9949, 0.9949, 0.9949,\n",
       "                0.9948, 0.9948, 0.9948, 0.9947, 0.9947, 0.9947, 0.9947, 0.9947, 0.9946,\n",
       "                0.9946, 0.9945, 0.9945, 0.9945, 0.9945, 0.9944, 0.9944, 0.9944, 0.9943,\n",
       "                0.9943, 0.9943, 0.9942, 0.9942, 0.9942, 0.9941, 0.9941, 0.9941, 0.9940,\n",
       "                0.9940, 0.9939, 0.9938, 0.9938, 0.9937, 0.9937, 0.9937, 0.9936, 0.9936,\n",
       "                0.9936, 0.9935, 0.9935, 0.9934, 0.9934, 0.9934, 0.9933, 0.9933, 0.9933,\n",
       "                0.9933, 0.9933, 0.9933, 0.9932, 0.9931, 0.9931, 0.9930, 0.9928, 0.9927,\n",
       "                0.9927, 0.9926, 0.9926, 0.9926, 0.9926, 0.9925, 0.9925, 0.9925, 0.9925,\n",
       "                0.9925, 0.9924, 0.9924, 0.9924, 0.9924, 0.9923, 0.9923, 0.9922, 0.9922,\n",
       "                0.9922, 0.9920, 0.9918, 0.9918, 0.9918, 0.9917, 0.9917, 0.9917, 0.9917,\n",
       "                0.9917, 0.9916, 0.9916, 0.9916, 0.9915, 0.9915, 0.9915, 0.9914, 0.9914,\n",
       "                0.9913, 0.9913, 0.9912, 0.9912, 0.9912, 0.9911, 0.9911, 0.9910, 0.9908,\n",
       "                0.9907, 0.9906, 0.9906, 0.9906, 0.9905, 0.9904, 0.9903, 0.9903, 0.9902,\n",
       "                0.9902, 0.9901, 0.9901, 0.9901, 0.9901, 0.9900, 0.9900, 0.9900, 0.9900,\n",
       "                0.9898, 0.9898, 0.9896, 0.9893, 0.9891, 0.9890, 0.9890, 0.9890, 0.9890,\n",
       "                0.9890, 0.9887, 0.9886, 0.9884, 0.9883, 0.9882, 0.9882, 0.9882, 0.9881,\n",
       "                0.9881, 0.9881, 0.9881, 0.9880, 0.9879, 0.9879, 0.9878, 0.9877, 0.9877,\n",
       "                0.9876, 0.9875, 0.9875, 0.9873, 0.9872, 0.9871, 0.9870, 0.9868, 0.9868,\n",
       "                0.9865, 0.9865, 0.9861, 0.9860, 0.9859, 0.9857, 0.9849, 0.9848, 0.9847,\n",
       "                0.9846, 0.9845, 0.9845, 0.9844, 0.9840, 0.9840, 0.9839, 0.9837, 0.9836,\n",
       "                0.9836, 0.9834, 0.9833, 0.9831, 0.9830, 0.9830, 0.9829, 0.9828, 0.9827,\n",
       "                0.9827, 0.9818, 0.9814, 0.9813, 0.9813, 0.9813, 0.9813, 0.9812, 0.9807,\n",
       "                0.9802, 0.9801, 0.9798, 0.9798, 0.9795, 0.9794, 0.9793, 0.9793, 0.9792,\n",
       "                0.9790, 0.9789, 0.9788, 0.9787, 0.9786, 0.9783, 0.9779, 0.9779, 0.9778,\n",
       "                0.9775, 0.9768, 0.9767, 0.9765, 0.9764, 0.9759, 0.9754, 0.9753, 0.9750,\n",
       "                0.9750, 0.9749, 0.9748, 0.9747, 0.9746, 0.9746, 0.9744, 0.9742, 0.9741,\n",
       "                0.9740, 0.9734, 0.9728, 0.9727, 0.9727, 0.9726, 0.9726, 0.9721, 0.9717,\n",
       "                0.9694, 0.9682, 0.9658, 0.9655, 0.9646, 0.9641, 0.9632, 0.9619, 0.9599,\n",
       "                0.9597, 0.9591, 0.9588, 0.9587, 0.9581, 0.9574, 0.9571, 0.9569, 0.9549,\n",
       "                0.9532, 0.9520, 0.9518, 0.9514, 0.9506, 0.9502, 0.9490, 0.9486, 0.9476,\n",
       "                0.9475, 0.9468, 0.9459, 0.9452, 0.9400, 0.9367, 0.9355, 0.9349, 0.9346,\n",
       "                0.9338, 0.9331, 0.9321, 0.9320, 0.9320, 0.9269, 0.9249, 0.9243, 0.9222,\n",
       "                0.9192, 0.9175, 0.9160, 0.9143, 0.9143, 0.9135, 0.9113, 0.9104, 0.9093,\n",
       "                0.9088, 0.9064, 0.9064, 0.9061, 0.9035, 0.9031, 0.9027, 0.8992, 0.8991,\n",
       "                0.8991, 0.8969, 0.8955, 0.8952, 0.8941, 0.8929, 0.8873, 0.8865, 0.8854,\n",
       "                0.8833, 0.8815, 0.8801, 0.8799, 0.8798, 0.8790, 0.8776, 0.8766, 0.8746,\n",
       "                0.8740, 0.8740, 0.8719, 0.8711, 0.8709, 0.8701, 0.8691, 0.8663, 0.8654,\n",
       "                0.8646, 0.8627, 0.8626, 0.8626, 0.8603, 0.8590, 0.8552, 0.8521, 0.8452,\n",
       "                0.8440, 0.8432, 0.8260, 0.8258, 0.8242, 0.8133, 0.8126, 0.8109, 0.8109,\n",
       "                0.8069, 0.8030, 0.8018, 0.8012, 0.7981, 0.7886, 0.7835, 0.7671, 0.7594,\n",
       "                0.7575, 0.7474, 0.7407, 0.7141, 0.7131, 0.6978, 0.6973, 0.6906, 0.6843,\n",
       "                0.6740, 0.6621, 0.6604, 0.6593, 0.6472, 0.6452, 0.6412, 0.6384, 0.6322,\n",
       "                0.6047, 0.5889, 0.5779, 0.5772, 0.5760, 0.5540, 0.5534, 0.5456, 0.5454,\n",
       "                0.5432, 0.5388, 0.5326, 0.5215, 0.5195, 0.5146, 0.5140, 0.5133, 0.5084,\n",
       "                0.5017, 0.4774, 0.4206, 0.4130, 0.4032, 0.3898, 0.3806, 0.3684, 0.3638,\n",
       "                0.3472, 0.3404, 0.3282, 0.3239, 0.3074, 0.2979, 0.2833, 0.2609, 0.2533],\n",
       "               device='cuda:0')\n",
       ") at 0x20d2fead510>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpn_results_list1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d8e3419-fe05-4b36-8bd8-d35103683fdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<DetDataSample(\n",
       " \n",
       "     META INFORMATION\n",
       "     img_path: 'C:\\\\Users\\\\Mikky\\\\Desktop\\\\树冠数据集\\\\第二次数据集\\\\Testing_set_phase1\\\\Testing_set\\\\images\\\\Dataset_4_val_0.png'\n",
       "     pad_shape: (1024, 1024)\n",
       "     batch_input_shape: (1024, 1024)\n",
       "     img_id: 0\n",
       "     img_shape: (1024, 1024)\n",
       "     scale_factor: (1.0, 1.0)\n",
       "     ori_shape: (1024, 1024)\n",
       " \n",
       "     DATA FIELDS\n",
       "     ignored_instances: <InstanceData(\n",
       "         \n",
       "             META INFORMATION\n",
       "         \n",
       "             DATA FIELDS\n",
       "             bboxes: tensor([], device='cuda:0', size=(0, 4))\n",
       "             labels: tensor([], device='cuda:0', dtype=torch.int64)\n",
       "             masks: BitmapMasks(num_masks=0, height=1024, width=1024)\n",
       "         ) at 0x20d2feac750>\n",
       "     gt_instances: <InstanceData(\n",
       "         \n",
       "             META INFORMATION\n",
       "         \n",
       "             DATA FIELDS\n",
       "             bboxes: tensor([], device='cuda:0', size=(0, 4))\n",
       "             labels: tensor([], device='cuda:0', dtype=torch.int64)\n",
       "             masks: BitmapMasks(num_masks=0, height=1024, width=1024)\n",
       "         ) at 0x20d2feacc10>\n",
       " ) at 0x20d2feac790>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7e2c1db-86e7-4397-85f0-36d7fe6fd04e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0,   0,   0],\n",
       "        [ 87, 112,  82],\n",
       "        [ 88, 110,  81],\n",
       "        ...,\n",
       "        [ 92, 105,  74],\n",
       "        [ 94, 105,  77],\n",
       "        [ 98, 105,  76]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [ 84, 110,  87],\n",
       "        [ 88, 114,  91],\n",
       "        ...,\n",
       "        [ 94, 104,  70],\n",
       "        [ 97, 104,  71],\n",
       "        [ 98, 101,  68]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [ 85, 108,  84],\n",
       "        [ 88, 110,  85],\n",
       "        ...,\n",
       "        [ 93, 101,  63],\n",
       "        [ 92, 102,  60],\n",
       "        [ 88,  99,  57]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [ 88, 111,  90],\n",
       "        [ 90, 111,  93],\n",
       "        ...,\n",
       "        [ 90, 105,  83],\n",
       "        [ 88, 102,  81],\n",
       "        [ 89, 104,  84]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [ 92, 114,  93],\n",
       "        [ 90, 113,  95],\n",
       "        ...,\n",
       "        [ 90, 105,  84],\n",
       "        [ 88, 102,  82],\n",
       "        [ 89, 102,  84]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [ 94, 119,  97],\n",
       "        [ 89, 115,  95],\n",
       "        ...,\n",
       "        [ 95, 109,  86],\n",
       "        [ 93, 106,  85],\n",
       "        [ 93, 103,  85]]], dtype=uint8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "245cc349-5ddb-46ce-8daa-fa42a424700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_slice=data_sample.pred_instances[valid_row_indices]\n",
    "img_slice=sliced_image_object[0][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5bb03ac0-cb8a-4d8e-a88a-98fcab5cb0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipeline=prepare_pipeline(model, img_slice)\n",
    "inputs, data_samples = prepare_model_inputs(img_slice, model,test_pipeline)\n",
    "with torch.no_grad():\n",
    "    x, image_embeddings, image_positional_embeddings = model.extract_feat(inputs)\n",
    "    rpn_results_list1 = model.rpn_head.predict(x, data_samples, rescale=True)\n",
    "    rpn_results_list=[sample_slice]\n",
    "    results_list = model.roi_head.predict(\n",
    "        x, rpn_results_list, data_samples, rescale=True,\n",
    "        image_embeddings=image_embeddings,\n",
    "        image_positional_embeddings=image_positional_embeddings,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc98a4-1f60-4710-ad9e-da4b1dab4daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_detections(img_slice,results_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c91864e-6e19-497e-8a6a-8fd6fcd445f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "test_pipeline=prepare_pipeline(model, img)\n",
    "\n",
    "inputs, data_samples = prepare_model_inputs(img, model,test_pipeline)\n",
    "\n",
    "data_samples.set_metainfo({\"ori_shape\":(1024,1024),\"scale_factor\":(1,1)})\n",
    "data_samples\n",
    "\n",
    "with torch.no_grad():\n",
    "    x, image_embeddings, image_positional_embeddings = model.extract_feat(inputs)\n",
    "    rpn_results_list1 = model.rpn_head.predict(x, data_samples, rescale=True)\n",
    "    # rpn_results_list=[data_sample.pred_instances]\n",
    "    results_list = model.roi_head.predict(\n",
    "        x, rpn_results_list, data_samples, rescale=True,\n",
    "        image_embeddings=image_embeddings,\n",
    "        image_positional_embeddings=image_positional_embeddings,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "45ad4419-d240-425a-9250-1753fa50815f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 256, 256])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555ba4f5-4140-48d8-b272-a816ac60d24a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e62f8924-e3f8-4237-97cc-de7cdbeae1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_detections(img,results_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ffe83af-0de0-44e0-9763-3ce3ed4f77c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DetDataSample(\n",
       "\n",
       "    META INFORMATION\n",
       "    pad_shape: (1024, 1024)\n",
       "    batch_input_shape: (1024, 1024)\n",
       "    img_shape: (1024, 1024)\n",
       "    img_path: None\n",
       "    img_id: 0\n",
       "    ori_shape: (341, 341)\n",
       "    scale_factor: (3.002932551319648, 3.002932551319648)\n",
       "\n",
       "    DATA FIELDS\n",
       "    pred_instances: <InstanceData(\n",
       "        \n",
       "            META INFORMATION\n",
       "        \n",
       "            DATA FIELDS\n",
       "            bboxes: tensor([[777.7944, 701.4855, 886.8213, 812.5743],\n",
       "                        [482.3196, 269.0388, 565.2842, 357.8298],\n",
       "                        [590.5324, 173.4079, 643.3612, 233.4373],\n",
       "                        ...,\n",
       "                        [558.0894, 568.8455, 582.6973, 594.2339],\n",
       "                        [675.6006, 371.4516, 712.3553, 393.8386],\n",
       "                        [541.1625, 683.0000, 563.7170, 692.1971]], device='cuda:0')\n",
       "            scores: tensor([0.9926, 0.9923, 0.9915, 0.9880, 0.9877, 0.9874, 0.9874, 0.9873, 0.9865,\n",
       "                        0.9863, 0.9863, 0.9852, 0.9834, 0.9834, 0.9828, 0.9825, 0.9815, 0.9813,\n",
       "                        0.9799, 0.9791, 0.9790, 0.9763, 0.9753, 0.9752, 0.9746, 0.9744, 0.9735,\n",
       "                        0.9721, 0.9705, 0.9701, 0.9692, 0.9662, 0.9657, 0.9651, 0.9644, 0.9630,\n",
       "                        0.9630, 0.9609, 0.9602, 0.9600, 0.9599, 0.9598, 0.9594, 0.9586, 0.9568,\n",
       "                        0.9556, 0.9550, 0.9543, 0.9534, 0.9524, 0.9524, 0.9519, 0.9507, 0.9498,\n",
       "                        0.9485, 0.9470, 0.9468, 0.9461, 0.9459, 0.9421, 0.9419, 0.9413, 0.9406,\n",
       "                        0.9402, 0.9373, 0.9372, 0.9355, 0.9349, 0.9339, 0.9297, 0.9288, 0.9280,\n",
       "                        0.9257, 0.9244, 0.9236, 0.9235, 0.9226, 0.9211, 0.9204, 0.9201, 0.9184,\n",
       "                        0.9117, 0.9101, 0.9087, 0.9073, 0.9050, 0.9027, 0.9017, 0.9015, 0.9009,\n",
       "                        0.9003, 0.8994, 0.8969, 0.8963, 0.8962, 0.8959, 0.8949, 0.8939, 0.8933,\n",
       "                        0.8902, 0.8877, 0.8874, 0.8866, 0.8851, 0.8850, 0.8844, 0.8825, 0.8813,\n",
       "                        0.8803, 0.8790, 0.8775, 0.8756, 0.8748, 0.8730, 0.8727, 0.8720, 0.8714,\n",
       "                        0.8713, 0.8712, 0.8700, 0.8678, 0.8675, 0.8668, 0.8651, 0.8636, 0.8630,\n",
       "                        0.8618, 0.8596, 0.8595, 0.8589, 0.8544, 0.8512, 0.8502, 0.8488, 0.8471,\n",
       "                        0.8435, 0.8427, 0.8424, 0.8423, 0.8394, 0.8384, 0.8318, 0.8316, 0.8313,\n",
       "                        0.8308, 0.8300, 0.8290, 0.8287, 0.8283, 0.8271, 0.8270, 0.8245, 0.8207,\n",
       "                        0.8204, 0.8204, 0.8159, 0.8158, 0.8149, 0.8143, 0.8131, 0.8108, 0.8102,\n",
       "                        0.8084, 0.8068, 0.8067, 0.8063, 0.8061, 0.8026, 0.8011, 0.8006, 0.8006,\n",
       "                        0.7952, 0.7925, 0.7878, 0.7873, 0.7850, 0.7799, 0.7796, 0.7793, 0.7756,\n",
       "                        0.7753, 0.7713, 0.7684, 0.7625, 0.7623, 0.7603, 0.7583, 0.7556, 0.7556,\n",
       "                        0.7552, 0.7539, 0.7536, 0.7528, 0.7511, 0.7363, 0.7348, 0.7292, 0.7239,\n",
       "                        0.7236, 0.7223, 0.7220, 0.7217, 0.7191, 0.7191, 0.7190, 0.7188, 0.7139,\n",
       "                        0.7138, 0.7109, 0.7089, 0.7000, 0.6983, 0.6925, 0.6883, 0.6874, 0.6857,\n",
       "                        0.6828, 0.6809, 0.6767, 0.6731, 0.6708, 0.6693, 0.6636, 0.6625, 0.6587,\n",
       "                        0.6554, 0.6506, 0.6432, 0.6425, 0.6423, 0.6399, 0.6353, 0.6244, 0.6235,\n",
       "                        0.6212, 0.6209, 0.6168, 0.6148, 0.6134, 0.6101, 0.6019, 0.6016, 0.6001,\n",
       "                        0.5999, 0.5977, 0.5969, 0.5934, 0.5932, 0.5907, 0.5881, 0.5796, 0.5762,\n",
       "                        0.5739, 0.5727, 0.5682, 0.5674, 0.5654, 0.5652, 0.5638, 0.5543, 0.5482,\n",
       "                        0.5474, 0.5467, 0.5462, 0.5418, 0.5397, 0.5384, 0.5376, 0.5374, 0.5364,\n",
       "                        0.5316, 0.5292, 0.5274, 0.5271, 0.5261, 0.5260, 0.5239, 0.5228, 0.5101,\n",
       "                        0.5095, 0.5089, 0.5089, 0.5078, 0.5047, 0.5020, 0.5001, 0.4990, 0.4989,\n",
       "                        0.4974, 0.4961, 0.4930, 0.4910, 0.4859, 0.4831, 0.4824, 0.4814, 0.4798,\n",
       "                        0.4767, 0.4750, 0.4727, 0.4668, 0.4656, 0.4648, 0.4611, 0.4553, 0.4552,\n",
       "                        0.4526, 0.4484, 0.4458, 0.4388, 0.4353, 0.4352, 0.4295, 0.4285, 0.4278,\n",
       "                        0.4261, 0.4260, 0.4233, 0.4228, 0.4212, 0.4193, 0.4188, 0.4182, 0.4181,\n",
       "                        0.4147, 0.4127, 0.4120, 0.4106, 0.4103, 0.4082, 0.4058, 0.4025, 0.4015,\n",
       "                        0.4013, 0.3987, 0.3945, 0.3943, 0.3894, 0.3858, 0.3835, 0.3789, 0.3786,\n",
       "                        0.3756, 0.3727, 0.3663, 0.3557, 0.3478, 0.3438, 0.3418, 0.3399, 0.3390,\n",
       "                        0.3374, 0.3362, 0.3355, 0.3347, 0.3330, 0.3220, 0.3161, 0.3140, 0.3133,\n",
       "                        0.3091, 0.3076, 0.3038, 0.3036, 0.3025, 0.3002, 0.2968, 0.2946, 0.2925,\n",
       "                        0.2880, 0.2870, 0.2841, 0.2820, 0.2801, 0.2799, 0.2790, 0.2776, 0.2761,\n",
       "                        0.2749, 0.2706, 0.2637, 0.2633, 0.2610, 0.2568, 0.2546, 0.2531, 0.2526,\n",
       "                        0.2512, 0.2503, 0.2485, 0.2479, 0.2471, 0.2465, 0.2460, 0.2432, 0.2417,\n",
       "                        0.2409, 0.2403, 0.2400, 0.2355, 0.2353, 0.2346, 0.2323, 0.2317, 0.2300,\n",
       "                        0.2263, 0.2259, 0.2252, 0.2245, 0.2238, 0.2199, 0.2196, 0.2182, 0.2178,\n",
       "                        0.2177, 0.2168, 0.2112, 0.2093, 0.2090, 0.2081, 0.2080, 0.2061, 0.2060,\n",
       "                        0.2059, 0.2033, 0.2021, 0.1998, 0.1993, 0.1986, 0.1963, 0.1954, 0.1941,\n",
       "                        0.1931, 0.1906, 0.1880, 0.1879, 0.1872, 0.1848, 0.1843, 0.1840, 0.1838,\n",
       "                        0.1818, 0.1812, 0.1807, 0.1791, 0.1767, 0.1765, 0.1761, 0.1750, 0.1711,\n",
       "                        0.1695, 0.1693, 0.1681, 0.1675, 0.1657, 0.1640, 0.1639, 0.1634, 0.1628,\n",
       "                        0.1618, 0.1599, 0.1590, 0.1588, 0.1586, 0.1543, 0.1540, 0.1536, 0.1528,\n",
       "                        0.1525, 0.1519, 0.1510, 0.1503, 0.1496, 0.1472, 0.1469, 0.1459, 0.1458,\n",
       "                        0.1456, 0.1450, 0.1442, 0.1439, 0.1438, 0.1435, 0.1432, 0.1430, 0.1422,\n",
       "                        0.1417, 0.1417, 0.1416, 0.1408, 0.1406, 0.1401, 0.1391, 0.1378, 0.1368,\n",
       "                        0.1361, 0.1357, 0.1348, 0.1327, 0.1324, 0.1315, 0.1308, 0.1296, 0.1293,\n",
       "                        0.1284, 0.1281, 0.1280, 0.1248, 0.1246, 0.1236, 0.1235, 0.1223, 0.1204,\n",
       "                        0.1196, 0.1184, 0.1175, 0.1174, 0.1172, 0.1163, 0.1152, 0.1150, 0.1148,\n",
       "                        0.1141, 0.1135, 0.1132, 0.1123, 0.1115, 0.1115, 0.1113, 0.1110, 0.1107,\n",
       "                        0.1102, 0.1098, 0.1080, 0.1072, 0.1070, 0.1061, 0.1059, 0.1048, 0.1043,\n",
       "                        0.1039, 0.1036, 0.1034, 0.1015, 0.1014, 0.1012, 0.1011, 0.1005, 0.0999,\n",
       "                        0.0994, 0.0981, 0.0981, 0.0969, 0.0967, 0.0960, 0.0953, 0.0951, 0.0940,\n",
       "                        0.0933, 0.0932, 0.0920, 0.0913, 0.0911, 0.0902, 0.0893, 0.0867, 0.0865,\n",
       "                        0.0863, 0.0861, 0.0860, 0.0842, 0.0840, 0.0838, 0.0836, 0.0836, 0.0834,\n",
       "                        0.0833, 0.0829, 0.0822, 0.0820, 0.0817, 0.0803, 0.0796, 0.0789, 0.0771,\n",
       "                        0.0770, 0.0770, 0.0754, 0.0752, 0.0743, 0.0740, 0.0739, 0.0739, 0.0739,\n",
       "                        0.0738, 0.0736, 0.0734, 0.0732, 0.0726, 0.0725, 0.0725, 0.0724, 0.0721,\n",
       "                        0.0721, 0.0718, 0.0697, 0.0695, 0.0684, 0.0680, 0.0679, 0.0676, 0.0657,\n",
       "                        0.0647, 0.0644, 0.0643, 0.0632, 0.0632, 0.0630, 0.0629, 0.0625, 0.0623,\n",
       "                        0.0619, 0.0613, 0.0611, 0.0600, 0.0599, 0.0598, 0.0594, 0.0575, 0.0575,\n",
       "                        0.0573, 0.0568, 0.0559, 0.0555, 0.0531, 0.0526, 0.0514, 0.0511, 0.0503],\n",
       "                       device='cuda:0')\n",
       "            labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
       "        ) at 0x255b3aef390>\n",
       "    gt_instances: <InstanceData(\n",
       "        \n",
       "            META INFORMATION\n",
       "        \n",
       "            DATA FIELDS\n",
       "            masks: BitmapMasks(num_masks=0, height=341, width=341)\n",
       "            bboxes: tensor([], device='cuda:0', size=(0, 4))\n",
       "            labels: tensor([], device='cuda:0', dtype=torch.int64)\n",
       "        ) at 0x255b3a813d0>\n",
       "    ignored_instances: <InstanceData(\n",
       "        \n",
       "            META INFORMATION\n",
       "        \n",
       "            DATA FIELDS\n",
       "            masks: BitmapMasks(num_masks=0, height=341, width=341)\n",
       "            bboxes: tensor([], device='cuda:0', size=(0, 4))\n",
       "            labels: tensor([], device='cuda:0', dtype=torch.int64)\n",
       "        ) at 0x25297bdd090>\n",
       ") at 0x255b3aa6d50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "# with torch.no_grad():\n",
    "#     # 提取特征\n",
    "#     x, image_embeddings, image_positional_embeddings = model.extract_feat(inputs)\n",
    "\n",
    "#     # 使用 RPN 生成建议框\n",
    "#     if data_samples[0].get('proposals', None) is None:\n",
    "#         rpn_results_list = model.rpn_head.predict(x, data_samples, rescale=False)\n",
    "#     else:\n",
    "#         rpn_results_list = [data_sample.proposals for data_sample in data_samples]\n",
    "\n",
    "#     # 使用处理后的建议框进行 Mask 预测\n",
    "#     results_list = model.roi_head.predict(\n",
    "#         x, rpn_results_list, data_samples, rescale=True,\n",
    "#         image_embeddings=image_embeddings,\n",
    "#         image_positional_embeddings=image_positional_embeddings,\n",
    "#     )\n",
    "\n",
    "#     # 添加预测结果到数据样本中\n",
    "#     data_samples = model.add_pred_to_datasample(data_samples, results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56750c1-b67d-40a9-b35d-13dbfb6d731f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "84a6a8ec-fe8c-499a-b395-5ed38cd6f827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing slices: 100%|███████████████████████████████████████████████████████████████| 25/25 [00:09<00:00,  2.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# import mmcv\n",
    "# from mmdet.utils.large_image import merge_results_by_nms\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# if isinstance(img, str):\n",
    "#     img = mmcv.imread(img)  # Load the image if a path is provided\n",
    "\n",
    "# import cv2\n",
    "# scale_factor=3\n",
    "# height, width=img.shape[:2]\n",
    "# from sahi.slicing import slice_image\n",
    "\n",
    "# sliced_image_object = slice_image(\n",
    "#     img,\n",
    "#     slice_height=round(1024/scale_factor),\n",
    "#     slice_width=round(1024/scale_factor),\n",
    "#     overlap_height_ratio=0.5, # 0.25,\n",
    "#     overlap_width_ratio=0.5 # 0.25\n",
    "# )\n",
    "# test_pipeline_np=prepare_pipeline(model, sliced_image_object.images[0])\n",
    "# slice_results=[]\n",
    "# for img_slice in tqdm(sliced_image_object.images, desc=\"Processing slices\"):\n",
    "#     # img_slice=sliced_image_object.images[0]        \n",
    "#     inputs, data_samples = prepare_model_inputs(img_slice,  model,test_pipeline_np)\n",
    "#     data_sample = inference_bbox(model, inputs, data_samples)[0]\n",
    "#     slice_results.append(data_sample)\n",
    "\n",
    "# image_result = merge_results_by_nms(\n",
    "#     slice_results,\n",
    "#     sliced_image_object.starting_pixels,\n",
    "#     src_image_shape=(height, width),\n",
    "#     nms_cfg={\n",
    "#         'type': \"nms\",\n",
    "#         'iou_threshold': 0.2,\n",
    "#         # 'max_num': 200\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "60435536-dbc5-4b6e-816f-1bb411f48f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_result = merge_results_by_nms(\n",
    "#     slice_results,\n",
    "#     sliced_image_object.starting_pixels,\n",
    "#     src_image_shape=(height, width),\n",
    "#     nms_cfg={\n",
    "#         'type': \"nms\",\n",
    "#         'iou_threshold': 0.4\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fc2b64-2b6d-4d43-87b0-08434985056d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ffb9a7d-3a8e-4b84-bd97-f0d7104a552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_slice=sliced_image_object.images[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0946fa4-6c24-4c6c-8d34-adf880a0eaeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_list = prepare_model_inputs(img_slice, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc2ed29c-6435-4cbb-ad20-6e60477f69de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1024, 1024])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8560d64-1b4c-427a-866a-17181f2a0e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3ba831fc-a71c-4c90-aa72-69455f570242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_bbox_results(img_slice,data_samples[0].pred_instances)\n",
    "visualize_bbox_results(img,image_result.pred_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72ce353f-66c4-4f7b-8b8a-ec593cba55e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmcv\n",
    "from sahi.slicing import slice_image\n",
    "if isinstance(img, str):\n",
    "    img = mmcv.imread(img)  # Load the image if a path is provided\n",
    "\n",
    "# # Enlarge the image according to the scale factor\n",
    "# img_enlarged = cv2.resize(img, (int(img.shape[1] * scale_factor), int(\n",
    "#     img.shape[0] * scale_factor)), interpolation=cv2.INTER_LINEAR)\n",
    "scale_factor=3\n",
    "# Slice the enlarged image\n",
    "sliced_image_object = slice_image(\n",
    "    img,\n",
    "    slice_height=round(1024/scale_factor),\n",
    "    slice_width=round(1024/scale_factor),\n",
    "    overlap_height_ratio=0.25, # 0.25,\n",
    "    overlap_width_ratio=0.25 # 0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6594cfb-603f-46bb-977a-aa9661a88164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': array([[[  0,   0,   0],\n",
       "         [ 87, 112,  82],\n",
       "         [ 88, 110,  81],\n",
       "         ...,\n",
       "         [ 92, 105,  74],\n",
       "         [ 94, 105,  77],\n",
       "         [ 98, 105,  76]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [ 84, 110,  87],\n",
       "         [ 88, 114,  91],\n",
       "         ...,\n",
       "         [ 94, 104,  70],\n",
       "         [ 97, 104,  71],\n",
       "         [ 98, 101,  68]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [ 85, 108,  84],\n",
       "         [ 88, 110,  85],\n",
       "         ...,\n",
       "         [ 93, 101,  63],\n",
       "         [ 92, 102,  60],\n",
       "         [ 88,  99,  57]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [ 88, 111,  90],\n",
       "         [ 90, 111,  93],\n",
       "         ...,\n",
       "         [ 90, 105,  83],\n",
       "         [ 88, 102,  81],\n",
       "         [ 89, 104,  84]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [ 92, 114,  93],\n",
       "         [ 90, 113,  95],\n",
       "         ...,\n",
       "         [ 90, 105,  84],\n",
       "         [ 88, 102,  82],\n",
       "         [ 89, 102,  84]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [ 94, 119,  97],\n",
       "         [ 89, 115,  95],\n",
       "         ...,\n",
       "         [ 95, 109,  86],\n",
       "         [ 93, 106,  85],\n",
       "         [ 93, 103,  85]]], dtype=uint8),\n",
       " 'coco_image': CocoImage<\n",
       "     id: None,\n",
       "     file_name: None_0_0_341_341.png,\n",
       "     height: 341,\n",
       "     width: 341,\n",
       "     annotations: List[CocoAnnotation],\n",
       "     predictions: List[CocoPrediction]>,\n",
       " 'starting_pixel': [0, 0],\n",
       " 'filename': 'None_0_0_341_341.png'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliced_image_object[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "785d23a0-e33a-4d12-9518-144f8c4ef6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': array([[[  0,   0,   0],\n",
       "         [ 87, 112,  82],\n",
       "         [ 88, 110,  81],\n",
       "         ...,\n",
       "         [ 92, 105,  74],\n",
       "         [ 94, 105,  77],\n",
       "         [ 98, 105,  76]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [ 84, 110,  87],\n",
       "         [ 88, 114,  91],\n",
       "         ...,\n",
       "         [ 94, 104,  70],\n",
       "         [ 97, 104,  71],\n",
       "         [ 98, 101,  68]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [ 85, 108,  84],\n",
       "         [ 88, 110,  85],\n",
       "         ...,\n",
       "         [ 93, 101,  63],\n",
       "         [ 92, 102,  60],\n",
       "         [ 88,  99,  57]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [ 88, 111,  90],\n",
       "         [ 90, 111,  93],\n",
       "         ...,\n",
       "         [ 90, 105,  83],\n",
       "         [ 88, 102,  81],\n",
       "         [ 89, 104,  84]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [ 92, 114,  93],\n",
       "         [ 90, 113,  95],\n",
       "         ...,\n",
       "         [ 90, 105,  84],\n",
       "         [ 88, 102,  82],\n",
       "         [ 89, 102,  84]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [ 94, 119,  97],\n",
       "         [ 89, 115,  95],\n",
       "         ...,\n",
       "         [ 95, 109,  86],\n",
       "         [ 93, 106,  85],\n",
       "         [ 93, 103,  85]]], dtype=uint8),\n",
       " 'coco_image': CocoImage<\n",
       "     id: None,\n",
       "     file_name: None_0_0_341_341.png,\n",
       "     height: 341,\n",
       "     width: 341,\n",
       "     annotations: List[CocoAnnotation],\n",
       "     predictions: List[CocoPrediction]>,\n",
       " 'starting_pixel': [0, 0]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliced_image_object._sliced_image_list[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e3207e1-d871-4df0-9345-8a07b8197ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Perform inference on the slices\n",
    "slice_results = []\n",
    "start = 0\n",
    "batch_size = 8\n",
    "piece_shape = (round(img.shape[0]/scale_factor),\n",
    "               round(img.shape[1]/scale_factor), img.shape[2])\n",
    "while True:\n",
    "    end = min(start + batch_size, len(sliced_image_object.images))\n",
    "    batch_slices = sliced_image_object.images[start:end]\n",
    "\n",
    "    \n",
    "    \n",
    "    # Inference\n",
    "    batch_results = inference_detector(model, batch_slices)\n",
    "\n",
    "    # Scale down the predictions for each batch result\n",
    "    for result in batch_results:\n",
    "        result = scale_detections(\n",
    "            result.pred_instances, scale_factor, piece_shape)\n",
    "        # result = filter_by_score(result, 0.4)  # Filter results with score > 0.4\n",
    "\n",
    "    slice_results.extend(batch_results)\n",
    "\n",
    "    if end >= len(sliced_image_object.images):\n",
    "        break\n",
    "    start += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2363631-a63f-428b-b9af-b8866cec3d78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "piece_shape = (round(img.shape[0]/scale_factor),\n",
    "               round(img.shape[1]/scale_factor), img.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99eb76b2-9879-4140-9101-d91c90e3869a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(341, 341, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piece_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a573826-38e5-491c-b7b0-623a53dce354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1023"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "341*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0470264-7acb-4a5d-8d11-cd9314ae24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_slice(img,model, scale_factor):\n",
    "    \"\"\"Function to handle large images, scale them up for slicing, inference, and then correctly scale and merge the results.\"\"\"\n",
    "\n",
    "    if isinstance(img, str):\n",
    "        img = mmcv.imread(img)  # Load the image if a path is provided\n",
    "\n",
    "    # Enlarge the image according to the scale factor\n",
    "    img_enlarged = cv2.resize(img, (int(img.shape[1] * scale_factor), int(\n",
    "        img.shape[0] * scale_factor)), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # Slice the enlarged image\n",
    "    sliced_image_object = slice_image(\n",
    "        img_enlarged,\n",
    "        slice_height=1024,\n",
    "        slice_width=1024,\n",
    "        overlap_height_ratio=0.25, # 0.25,\n",
    "        overlap_width_ratio=0.25 # 0.25\n",
    "    )\n",
    "\n",
    "    # Perform inference on the slices\n",
    "    slice_results = []\n",
    "    start = 0\n",
    "    batch_size = 8\n",
    "    small_shape = (round(img.shape[0]/scale_factor),\n",
    "                   round(img.shape[1]/scale_factor), img.shape[2])\n",
    "    while True:\n",
    "        end = min(start + batch_size, len(sliced_image_object.images))\n",
    "        batch_slices = sliced_image_object.images[start:end]\n",
    "\n",
    "        # Inference\n",
    "        batch_results = inference_detector(model, batch_slices)\n",
    "\n",
    "        # Scale down the predictions for each batch result\n",
    "        for result in batch_results:\n",
    "            result = scale_down_detections(\n",
    "                result.pred_instances, scale_factor, small_shape)\n",
    "            # result = filter_by_score(result, 0.4)  # Filter results with score > 0.4\n",
    "\n",
    "        slice_results.extend(batch_results)\n",
    "\n",
    "        if end >= len(sliced_image_object.images):\n",
    "            break\n",
    "        start += batch_size\n",
    "\n",
    "    # Scale down the starting pixels for merging results\n",
    "    starting_pixels = np.array(sliced_image_object.starting_pixels)\n",
    "    scaled_starting_pixels = starting_pixels / scale_factor\n",
    "\n",
    "    # Convert the starting pixels into integer format if necessary\n",
    "    scaled_starting_pixels = np.round(\n",
    "        scaled_starting_pixels).astype(int).tolist()\n",
    "\n",
    "    # Merge results with Non-Maximum Suppression (NMS)\n",
    "    image_result = merge_results_by_nms(\n",
    "        slice_results,\n",
    "        scaled_starting_pixels,\n",
    "        src_image_shape=(img.shape[1], img.shape[0]),  # Original image shape\n",
    "        nms_cfg={'type': \"nms\", 'iou_threshold': 0.2}\n",
    "    )\n",
    "\n",
    "    return image_result.pred_instances\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
